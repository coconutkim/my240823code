{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
       "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
       "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
       "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
       "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
       "\n",
       "         9   ...      51      52      53      54      55      56      57  \\\n",
       "0    0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1    0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2    0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3    0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4    0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.2684  ...  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115   \n",
       "204  0.2154  ...  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032   \n",
       "205  0.2529  ...  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138   \n",
       "206  0.2354  ...  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079   \n",
       "207  0.2354  ...  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036   \n",
       "\n",
       "         58      59  60  \n",
       "0    0.0090  0.0032   0  \n",
       "1    0.0052  0.0044   0  \n",
       "2    0.0095  0.0078   0  \n",
       "3    0.0040  0.0117   0  \n",
       "4    0.0107  0.0094   0  \n",
       "..      ...     ...  ..  \n",
       "203  0.0193  0.0157   1  \n",
       "204  0.0062  0.0067   1  \n",
       "205  0.0077  0.0031   1  \n",
       "206  0.0036  0.0048   1  \n",
       "207  0.0061  0.0115   1  \n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./data/sonar3.csv',header=None)\n",
    "# 광석 여부 판별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./data/sonar3.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,:60]\n",
    "y=df.iloc[:,60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 60)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kfold\n",
    "\n",
    "사이킷런(scikit-learn)에서 제공\n",
    "\n",
    "교차 검증(cross-validation) 방법 중 하나로, 주어진 데이터셋을 K개의 폴드(fold)로 나누어 모델의 성능을 평가하는 데 사용됩니다\n",
    "\n",
    "KFold는 데이터를 훈련 세트와 테스트 세트로 나누는 작업을 K번 반복하여, 각 폴드마다 다른 부분이 테스트 세트로 사용되도록 합니다\n",
    "\n",
    "모델이 데이터의 특정 부분에 과적합되지 않도록 도와주며, 모델의 일반화 성능을 더 잘 평가할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class sklearn.model_selection.KFold(n_splits=5, *,\n",
    "# shuffle=False, random_state=None)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold=KFold(n_splits=5,shuffle=True,random_state=42)\n",
    "\n",
    "# split 데이터를 5등분하여 각 폴드를 한 번씩 테스트 세트로 사용한다\n",
    "# shuffle 데이터가 정렬된 순서에 영향을 받지 않기 위해 무작위로 섞는다\n",
    "# random state 랜덤성을 제어하고 42는 임의의 숫자이며 다른 숫자를 사용할 수도 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _BaseKFold.split at 0x0000022505A7A200>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold.split(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8  10  11  12  13  14  17  19  20  21\n",
      "  22  23  24  26  27  28  29  31  32  33  34  35  36  37  38  39  40  41\n",
      "  42  43  44  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60\n",
      "  61  62  63  64  67  70  71  72  74  76  77  78  79  80  81  82  83  85\n",
      "  86  87  88  89  90  91  92  93  94  97  99 102 103 105 106 107 108 109\n",
      " 110 111 112 114 115 116 117 118 119 121 122 123 126 127 128 129 130 131\n",
      " 132 133 134 136 137 139 140 142 143 144 145 146 149 150 151 152 153 156\n",
      " 157 158 159 160 162 163 165 168 169 172 173 174 175 176 177 179 181 182\n",
      " 183 184 186 187 188 189 191 192 193 194 195 196 198 199 200 201 202 203\n",
      " 204 205 206 207] [  9  15  16  18  25  30  45  65  66  68  69  73  75  84  95  96  98 100\n",
      " 101 104 113 120 124 125 135 138 141 147 148 154 155 161 164 166 167 170\n",
      " 171 178 180 185 190 197]\n",
      "size 166 42\n",
      "========================================================================\n",
      "[  0   1   2   3   4   6   7   8   9  10  11  13  14  15  16  17  18  20\n",
      "  21  22  23  25  27  30  32  33  34  36  37  39  40  43  44  45  46  47\n",
      "  48  49  50  52  53  54  57  58  59  61  62  63  64  65  66  68  69  70\n",
      "  71  72  73  74  75  77  80  81  83  84  85  87  88  89  90  91  92  94\n",
      "  95  96  98  99 100 101 102 103 104 105 106 107 109 110 111 112 113 114\n",
      " 116 118 119 120 121 122 123 124 125 126 129 130 131 133 134 135 136 137\n",
      " 138 140 141 143 144 145 147 148 149 150 151 152 153 154 155 156 157 159\n",
      " 160 161 162 163 164 165 166 167 169 170 171 172 173 174 175 178 179 180\n",
      " 182 183 184 185 186 187 188 189 190 191 192 193 196 197 198 199 200 201\n",
      " 202 204 206 207] [  5  12  19  24  26  28  29  31  35  38  41  42  51  55  56  60  67  76\n",
      "  78  79  82  86  93  97 108 115 117 127 128 132 139 142 146 158 168 176\n",
      " 177 181 194 195 203 205]\n",
      "size 166 42\n",
      "========================================================================\n",
      "[  1   3   5   7   8   9  12  13  14  15  16  17  18  19  20  21  23  24\n",
      "  25  26  28  29  30  31  34  35  37  38  39  40  41  42  43  44  45  47\n",
      "  48  49  50  51  52  53  54  55  56  57  58  59  60  63  64  65  66  67\n",
      "  68  69  71  72  73  74  75  76  78  79  80  81  82  83  84  86  87  88\n",
      "  89  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 110 113 115 116 117 120 121 123 124 125 127 128 129 130 131 132 133\n",
      " 134 135 136 138 139 141 142 144 146 147 148 149 151 153 154 155 157 158\n",
      " 160 161 163 164 166 167 168 169 170 171 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 188 189 190 191 193 194 195 196 197 200 201 203\n",
      " 204 205 206 207] [  0   2   4   6  10  11  22  27  32  33  36  46  61  62  70  77  85  90\n",
      " 109 111 112 114 118 119 122 126 137 140 143 145 150 152 156 159 162 165\n",
      " 172 187 192 198 199 202]\n",
      "size 166 42\n",
      "========================================================================\n",
      "[  0   1   2   4   5   6   9  10  11  12  14  15  16  18  19  20  21  22\n",
      "  24  25  26  27  28  29  30  31  32  33  35  36  37  38  41  42  45  46\n",
      "  48  50  51  52  54  55  56  57  58  60  61  62  63  65  66  67  68  69\n",
      "  70  71  73  74  75  76  77  78  79  82  84  85  86  87  88  90  92  93\n",
      "  95  96  97  98  99 100 101 102 103 104 106 107 108 109 111 112 113 114\n",
      " 115 116 117 118 119 120 121 122 124 125 126 127 128 129 130 132 134 135\n",
      " 137 138 139 140 141 142 143 145 146 147 148 149 150 151 152 154 155 156\n",
      " 157 158 159 160 161 162 164 165 166 167 168 169 170 171 172 173 174 176\n",
      " 177 178 179 180 181 185 187 188 190 191 192 194 195 196 197 198 199 200\n",
      " 202 203 204 205 207] [  3   7   8  13  17  23  34  39  40  43  44  47  49  53  59  64  72  80\n",
      "  81  83  89  91  94 105 110 123 131 133 136 144 153 163 175 182 183 184\n",
      " 186 189 193 201 206]\n",
      "size 167 41\n",
      "========================================================================\n",
      "[  0   2   3   4   5   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
      "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  38  39  40\n",
      "  41  42  43  44  45  46  47  49  51  53  55  56  59  60  61  62  64  65\n",
      "  66  67  68  69  70  72  73  75  76  77  78  79  80  81  82  83  84  85\n",
      "  86  89  90  91  93  94  95  96  97  98 100 101 104 105 108 109 110 111\n",
      " 112 113 114 115 117 118 119 120 122 123 124 125 126 127 128 131 132 133\n",
      " 135 136 137 138 139 140 141 142 143 144 145 146 147 148 150 152 153 154\n",
      " 155 156 158 159 161 162 163 164 165 166 167 168 170 171 172 175 176 177\n",
      " 178 180 181 182 183 184 185 186 187 189 190 192 193 194 195 197 198 199\n",
      " 201 202 203 205 206] [  1  14  20  21  37  48  50  52  54  57  58  63  71  74  87  88  92  99\n",
      " 102 103 106 107 116 121 129 130 134 149 151 157 160 169 173 174 179 188\n",
      " 191 196 200 204 207]\n",
      "size 167 41\n",
      "========================================================================\n"
     ]
    }
   ],
   "source": [
    "for train_index,test_index in kfold.split(X):\n",
    "    print(train_index,test_index)\n",
    "    print('size',len(train_index),len(test_index))\n",
    "    print('========================================================================')\n",
    "# 첫 번째 대괄호는 훈련용, 두 번째는 테스트용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 60)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "203    1\n",
       "204    1\n",
       "205    1\n",
       "206    1\n",
       "207    1\n",
       "Name: 60, Length: 208, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    model=Sequential()\n",
    "    model.add(Dense(24,input_dim=60,activation='relu'))\n",
    "    model.add(Dense(10,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4756 - accuracy: 0.8333\n",
      "accuracy of test: 0.8333333134651184\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.0304 - accuracy: 0.7857\n",
      "accuracy of test: 0.7857142686843872\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4642 - accuracy: 0.8571\n",
      "accuracy of test: 0.8571428656578064\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.8022 - accuracy: 0.8293\n",
      "accuracy of test: 0.8292682766914368\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x00000225095BE700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1386 - accuracy: 0.9268\n",
      "accuracy of test: 0.9268292784690857\n"
     ]
    }
   ],
   "source": [
    "for train_index,test_index in kfold.split(X):\n",
    "    # 주어진 데이터셋 x를 훈련과 테스트 세트로 나눌 수 있는 인덱스 생성\n",
    "\n",
    "    X_train, X_test=X.iloc[train_index,:],X.iloc[test_index,:]\n",
    "    # iloc[rows, columns]\n",
    "    # train_index에 해당하는 행을 선택하고, 모든 열(:)을 선택합니다\n",
    "    # 훈련용과 테스트용으로 나눈다\n",
    "\n",
    "    y_train, y_test=y.iloc[train_index],y.iloc[test_index]\n",
    "    model=model_fn()\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                #   모델을 컴파일한다=\n",
    "                #   학습시키기 전에 필요한 설정을 정의하고\n",
    "                #   모델을 준비하는 과정이다\n",
    "\n",
    "              optimizer='adam',metrics=['accuracy'])\n",
    "    model.fit(X_train,y_train,epochs=200,batch_size=10,verbose=0)\n",
    "    # 모델을 학습시킨다\n",
    "    # 훈련 데이터, 200번의 에포크 동안, 미니 배치 크기를 10으로 해서,\n",
    "    # 훈련 과정의 출력은 없다\n",
    "    \n",
    "    accuracy=model.evaluate(X_test,y_test)\n",
    "    print('accuracy of test:',accuracy[1])\n",
    "    accuracy_score.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.47559839487075806, 0.8333333134651184],\n",
       " [1.0304309129714966, 0.7857142686843872],\n",
       " [0.46417471766471863, 0.8571428656578064],\n",
       " [0.8021553754806519, 0.8292682766914368],\n",
       " [0.13855373859405518, 0.9268292784690857]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score\n",
    "# 각 폴드에 대한 정확도 점수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_s=[i[1] for i in accuracy_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8464576005935669"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(a_s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldltest1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
